{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introductory-probe",
   "metadata": {},
   "source": [
    "# ML Assignment 1 : Naive-Bayes Classifier\n",
    "### Anirudh Agrawal: 2018A7PS0099H | Aviral Agarwal: 2018A7PS0192H | Vikramjeet Das: 2018A7PS0280H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faced-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from preprocessing import download_nltk_deps, process_string\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "together-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(fname):\n",
    "    \"\"\"\n",
    "    Parses the given dataset and extracts out the emails and their associated class\n",
    "\n",
    "        Parameters:\n",
    "            fname : Name of dataset file\n",
    "\n",
    "        Returns:\n",
    "            (X,y) : List of processed emails and their classes\n",
    "    \"\"\"\n",
    "    X = list()\n",
    "    y = list()\n",
    "    data_file = open(fname, \"r\")\n",
    "    row = 0\n",
    "\n",
    "    for line in data_file:\n",
    "        line = line.strip()  # every line ends with a '\\n'\n",
    "        spam = int(line[-1])  # last character of every line is the class\n",
    "        email = line[:-1]\n",
    "        words = list(\n",
    "            set(process_string(email))\n",
    "        )  # consider only single occurence of each word in an email\n",
    "        X.append(words)\n",
    "        y.append(spam)\n",
    "        row += 1\n",
    "\n",
    "    data_file.close()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def shuffle(X, y):\n",
    "    \"\"\"\n",
    "    Shuffles rows of a dataframe and returns shuffled dataframe\n",
    "    \"\"\"\n",
    "    permute = np.random.permutation(len(X))\n",
    "    return X[permute], y[permute]\n",
    "\n",
    "\n",
    "def train_test_split(X, y, folds=7):\n",
    "    \"\"\"\n",
    "    Splits data into train-test split\n",
    "\n",
    "        Parameters:\n",
    "            X : Features\n",
    "            y : Labels\n",
    "            folds: number of partitions required for cross validation technique\n",
    "\n",
    "        Returns:\n",
    "            (X_train, y_train, X_test, y_test) : Train test split\n",
    "    \"\"\"\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.array(X, dtype=\"object\")\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.array(y)\n",
    "\n",
    "    X, y = shuffle(X, y)\n",
    "\n",
    "    for fold in range(0, folds):\n",
    "        test_indices = np.array(\n",
    "            [\n",
    "                x >= int(fold * len(X) / folds) and x < int((fold + 1) * len(X) / folds)\n",
    "                for x in range(0, len(X))\n",
    "            ]\n",
    "        )\n",
    "        yield X[~test_indices], y[~test_indices], X[test_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fiscal-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes:\n",
    "    def __init__(self):\n",
    "        self.params = dict()  # dictionary to store parameter values\n",
    "        self.p_spam = 0  # probability of spam emails\n",
    "        self.p_ham = 0  # probability of ham emails\n",
    "\n",
    "    def train(self, X, y):\n",
    "        assert X.shape[0] == y.shape[0], \"Data and target count do not match\"\n",
    "        assert len(np.unique(y)) == 2, \"Targets cannot have more than 2 classes\"\n",
    "        c_spam = 0  # count of spam emails\n",
    "        for words, spam in zip(X, y):\n",
    "            if spam:\n",
    "                c_spam += 1\n",
    "            for word in words:\n",
    "                if self.params.__contains__(word):\n",
    "                    self.params[word][spam] += 1\n",
    "                else:\n",
    "                    self.params[word] = (\n",
    "                        [1, 2] if spam else [2, 1]\n",
    "                    )  # Initialized to non-zero in accordance with Laplace Smoothening\n",
    "        self.p_spam = c_spam / len(X)\n",
    "        self.p_ham = 1 - self.p_spam\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = [0 for x in range(0, len(X))]\n",
    "        for ind in range(0, len(X)):\n",
    "            words = X[ind]\n",
    "            pred_spam = math.log(self.p_spam)\n",
    "            pred_ham = math.log(self.p_ham)\n",
    "            for word in words:\n",
    "                if self.params.__contains__(word):\n",
    "                    denominator = (\n",
    "                        self.params[word][0] + self.params[word][1] + 2\n",
    "                    )  # Added 2 in accordance with Laplace Smoothening\n",
    "                    pred_spam += math.log(self.params[word][1] / denominator)\n",
    "                    pred_ham += math.log(self.params[word][0] / denominator)\n",
    "                else:\n",
    "                    pass  # Ignore words which aren't encountered even once in the training dataset\n",
    "            preds[ind] = 1 if pred_spam >= pred_ham else 0\n",
    "        return preds\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return np.sum(preds == y) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wrong-frost",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aviral/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aviral/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each fold=  [0.7816901408450704, 0.8391608391608392, 0.7832167832167832, 0.7692307692307693, 0.7762237762237763, 0.8321678321678322, 0.8251748251748252]\n",
      "Average Accuracy=  0.8009807094314138\n"
     ]
    }
   ],
   "source": [
    "download_nltk_deps()\n",
    "X, y = fetch_data(\"dataset_NB.txt\")\n",
    "split = train_test_split(X, y, 7)\n",
    "nb = Naive_Bayes()\n",
    "acc = list()\n",
    "for i in range(0, 7):\n",
    "    X_train, y_train, X_test, y_test = split.__next__()\n",
    "    nb.__init__()\n",
    "    nb.train(X_train, y_train)\n",
    "    acc.append(nb.evaluate(X_test, y_test))\n",
    "print(\"Accuracy for each fold= \", acc)\n",
    "print(\"Average Accuracy= \", sum(acc) / 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
